{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =====Dealing with large data sets====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent\n",
    "Assume we have a cost function\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum_{i=1}^mcost(\\theta,(x^{(i)},y^{(i)})),$$\n",
    "where the $cost$ function can be, for example\n",
    "$$cost(\\theta,(x^{(i)},y^{(i)}))=\\frac{1}{2}(h_{\\theta}(x^{(i)})-y^{(i)})^2.$$\n",
    "Assume now that we have a large number of data ($m\\sim 10^7$ or greater) and we want to apply gradient descent\n",
    "$$\\theta_j=\\theta_j-\\frac{\\alpha}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x^{(i)}_j,$$\n",
    "for each $j$. In such a case the computation of the gradient at each epoch migth take an great amount of time due to the sumation over $i$.<br><br>\n",
    "\n",
    "The *stochastic gradient descent* algorithm aims to speedup the process by using a single data point at each epoch to compute the gradient of the cost function. In order to avoir a bias due to the order of the data, the data are first randomly shuffled, hence the name *stochastic*. We have then to iterate for all $j$\n",
    "$$\\theta_j=\\theta_j-\\alpha(h_{\\theta}(x^{(i)})-y^{(i)})x^{(i)}_j,$$\n",
    "with a single data point $i$ and then move to the next (random) value of $i$. With this algorithm one ussually does from 1 to 10 rounds over the data ($i$), depending on $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "This is a generalization of stochastic gradient descent where, instead of using a single data point at each epoch, one uses a small batch of data of size $b$. In this way, after shuffling the data we divide it into groups of size $b\\ll m$ and then use a different mini-batch $g$ at each calculation of the gradient descent\n",
    "$$\\theta_j=\\theta_j-\\frac{\\alpha}{b}\\sum_{i=1}^b(h_{\\theta}(x^{(i,g)})-y^{(i,g)})x^{(i,g)}_j,$$\n",
    "for every $j$, until we have gone through all the mini-batches $g$ one or more times. The original stochastic gradient descent is a special case of the mini-batch gradient descent when $b=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "To check for the convergence of the algorithm, what is ussually done instead of computing\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum_{i=1}^mcost(\\theta,(x^{(i)},y^{(i)})),$$\n",
    "at every step, is to take the average of $cost(\\theta,(x^{(i)},y^{(i)}))$ over a number $n$ of epochs and use that as a proxy to $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online learning\n",
    "When one has a continuous streaming of data it is possible to use each new data point to update the value of our parameters by using this single new data point $(x,y)$ to apply gradient descent to our existing parameters $\\theta$ as\n",
    "$$\\theta_j=\\theta_j-\\alpha(h_{\\theta}(x)-y)x_j,$$\n",
    "for all $j$. In this way the algorithm continuously updates the parameters and does not use the same point twice. Such an algorithm can adapt to a changing system (for example, changing user preferences).\n",
    "However, if the amount of data is limited, it is better to collect it all and then train the algorithm in a regular fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map-reduce\n",
    "Another way to cope with large amounts of data is to exploit data parallelism, as the summations needed to compute $J$ and its gradient can be divided into $n$ chunks, splitting the summation and giving each chunk to be summed in parallel by $n$ different computing units. Aftre each computed unid has finished its work, the results are added in a reduction operation to obtain the final value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===== Machine learning pipeline ====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photo OCR\n",
    "Photo Optical Character Recognition is the problem of identifying and deciphering text found in photos or other images. It is an example of a *machine learning pipeline*, where oe or more machine learning solutions are concatenated, probably along with non-machine learning solutions. In the OCR example we need first to locate the sections of the image that contain text, then for each section we have to segment the locations of each character, then we hace to identify each character and probably make some spell checking correction at the end. Each of these steps has to be carried out in order, and each one is in itself a problem which can be solved using a particular machine learning algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text detection\n",
    "One can use a supervised learning algorithm which uses a training set composed of patches of images that contain text and patches that don't, to differentiate between these two cases. These training patches would have to be of the same size and probably labeled by a human.<br>\n",
    "No we can scan an image using a sliding window with the same shape of the training patch, and select the regions that are recognized as text with a high probability. As not all the text might have the same size, we will have to scan the image several times using different window sizes, preserving the aspect ratio and scaling to the training patch size.<br>\n",
    "Once we have detected all the patches that contain text with a high probability, we can expand these regions a bit to join adjacent patches and obtain the complete regions that contain text in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character segmentation\n",
    "Now that we have the patches containing the text images, we can proceede to segment then in such a way that each segment contains a single character. To do this we might, for example, train a new classifier to identify images that contain whole single characters or white space from images that contain incomplete or mixed characters. These new system can now be applied to a sliding window that runs through each text patch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character classification\n",
    "Finally, we can use a third classifier that has bebn trained to distinguish the different characters on each of the segments obtained above, to obtain the actual text written in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial data\n",
    "Obtaining enough data to properly train a classifier can be problematic, and in some cases it is possible to construct new data artificially. For the OCR problem one might, for example, create text using different computer fonts and paste them over random backgrounds, using possibly some bluring, to syntetize more data to use as training material. One could also take original images an apply transformations to distort them and create new training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ceiling analysis\n",
    "Which part of the pipeline should you spend more time trying to improve? It is important to check, using some quality meassure, which part of the system is the most innacurate. To do this it is important to provide each part, individually, with correct imput data (as if the previous step in the pipeline has been done with 100% accuracy) andchech for the individual part's accuracy. Then, one can estimate how much does the overall performance of the system increases one une particular section gets 100% accuracy. The best part of the system to spend time improving is the one that improves the most the overall performance when its oun accuracy is increased to 100%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
