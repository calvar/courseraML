{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering\n",
    "The goal of the k-means clustering is quite simple: Given a set of points $x_i\\in\\mathbb{R}^d$ $i=1,\\dots,m$, find the cluster centroids $\\mu_k\\in\\mathbb{R}^d$ $k=1,\\dots,K$, such that\n",
    "$$J(\\mu)=\\frac{1}{m}\\sum_{i=1}^m|x_i-\\mu_{c(i)}|^2,$$\n",
    "is minimum, where $c(i)$ denotes the cluster where $x_i$ belongs.<br> \n",
    "<br>\n",
    "The algorithm used to achieve approximately this goal is the following:\n",
    "1. Initialize the centroids of the $K$ clusters.\n",
    "2. Set the cluster belonging index $c(i)=k$ for every $x_i$ by asignning each point to the cluster $k$ which is closest to it. That it the one that minimizes $|x_i-\\mu_k|$.\n",
    "3. Move each cluster $\\mu_k$ to the mean position of all the $x_i$ such that $c(i)=k$. \n",
    "4. If $\\mu_k=\\mu_{c(i)}$ for all clusters stop. Else go to 2.\n",
    "\n",
    "Note that this algorithm does not assure that the global minimum of $J$ is reached, as it can get stuck into a local minumum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis (PCA)\n",
    "The goal of PCA is to reduce the dimensionality of the problem by projecting the points $x_i\\in\\mathbb{R}^D$ on a subspace $U$ in $\\mathbb{R}^M$, where $M<D$ and the perpendicular distance of every $x_i$ to said subspace is as small as possible. Such a process can be considered to be a compression, and the idea is to lose as little information from the original data as possible while reducing the degrees of freedom of the problem. That is, if $z_i$ are the compressed coordinates (projections over the subspace $U$) and $\\tilde{x}_i$ the uncompressed coordinates, we have $x_i\\rightarrow z_i\\rightarrow \\tilde{x}_i$, and we want to minimize $|x_i-\\tilde{x}_i|^2$.<br>\n",
    "Let $B$ be the projection operator, which lives in $\\mathbb{R}^{D\\times M}$, then\n",
    "$$z_i=B^Tx_i,$$\n",
    "and\n",
    "$$\\tilde{x}_i=Bz_i.$$\n",
    "Retaining most information after data compression is equivalent to capturing the largest amount of variance in the low-dimensional code. The data covariance matrix, for data with mean $\\mu=0$ is given by\n",
    "$$\\text{Cov}[x]=\\frac{1}{m}\\sum_{i=1}^mx_ix^T_i,$$\n",
    "and tough the variance does not depend on the mean it is convenient to center the data around 0 before compressing it to make calculations easier. <br><br>\n",
    "\n",
    "The $M$ columns of the projection matrix are the basis vectors for the subspace $U$ in the original space\n",
    "$$B=[b_1,b_2,\\dots,b_{M}],$$\n",
    "where $b_l\\in\\mathbb{R}^D$ and $|b_l|^2=1$.<br><br>\n",
    "\n",
    "Now, the variance of the data along the axis descibed by vector $b_1$ is given by\n",
    "\\begin{align}\n",
    "V_1&=\\frac{1}{m}\\sum_{i=1}^mz_{1,i}^2\\\\\n",
    "&=\\frac{1}{m}\\sum_{i=1}^m(b_1^Tx_i)^2\\\\\n",
    "&=\\frac{1}{m}\\sum_{i=1}^mb_1^Tx_ix_i^Tb_1\\\\\n",
    "&=b_1^T\\text{Cov}[x]b_1,\n",
    "\\end{align}\n",
    "which gives us the optimization problem\n",
    "\\begin{align}\n",
    "&\\max_{b_1}b_1^T\\text{Cov}[x]b_1,\\\\\n",
    "&\\text{subject to }|b_1|^2=1.\n",
    "\\end{align}\n",
    "Using the method of Lagrange multipliers we obtain the lagrangian\n",
    "$$\\mathcal{L}(b_1,\\lambda_1)=b_1^T\\text{Cov}[x]b_1+\\lambda_1(1-b_1^Tb_1),$$\n",
    "which after differentiating and equationg to 0 gives the equations\n",
    "\\begin{align}\n",
    "\\text{Cov}[x]b_1&=\\lambda_1 b_1,\\\\\n",
    "b_1^Tb_1&=1,\n",
    "\\end{align}\n",
    "which shows that $b_1$ is an eigenvector of the centered data covariance matrix, with an eigenvalue equal to the variance $\\lambda_1=V_1$. Therefore, to maximize the variance we have to look for the eigenvector of the data covariance with the largest eigenvalue. This eigenvector is called *first principal component*. To estimate the contribution of the principal component we can obtain \n",
    "$$\\frac{1}{m}\\sum_{i=1}^m|\\tilde{x}_i-x_i|^2,$$\n",
    "where $\\tilde{x}_i=b_1z_{i,1}$.<br><br>\n",
    "\n",
    "We can also find an $M$ dimensional subspace in which variance is maximized. Assume we have already found the first $l-1$ principal components.\n",
    "Define the centered data matrix as\n",
    "$$X=[x_1,x_2,\\dots,x_m]\\in\\mathbb{R}^{D\\times m},$$\n",
    "and the projection operator\n",
    "$$B_{l-1}=\\sum_{k=1}^{l-1}b_kb_k^T.$$\n",
    "Then, obtain a new data matrix is\n",
    "$$\\hat{X}=X-B_{l-1}X,$$\n",
    "which holds the information that is left out after the projection onto the subspace of dimension $l-1$ is made. To find the $l$th principal component we have to fint the eigenvector of $\\hat{X}$ with the highest eigenvalue $$V_l=b_l^T\\hat{X}\\hat{X}^Tb_l=b_l^T\\text{Cov}[\\hat{X}]b_l,$$\n",
    "with the constraint $|b_l|^2=1$, which is also an eigenvector of $\\text{Cov}[X]$. Eigenvectors $V_1,\\dots,V_{l-1}$ of $\\text{Cov}[X]$ are also eigenvectors of $\\text{Cov}[\\hat{X}]$, but with eigenvalue 0. The maximum amount of variance captured by the PCA in the $M$th dimensional subspace is\n",
    "$$V_M=\\sum_{k=1}^M\\lambda_k$$\n",
    "where $\\lambda_k$ are the $M$ largest eigenvalues of $\\text{Cov}[X]$. The variance lost due to the compression via PCA is\n",
    "$$J=\\sum_{j=M+1}^D\\lambda_j=V_D-V_M,$$\n",
    "but sometimes people also use the relative loss\n",
    "$$J_r=\\frac{V_D-V_M}{V_D}=1-\\frac{V_M}{V_D}.$$\n",
    "<br>\n",
    "### Steps of PCA\n",
    "1. Mean substraction to have the data centered.\n",
    "2. Standarization to have the data dimensions in the same range.\n",
    "3. Eigendecomposition of the covariance matrix.\n",
    "4. Projection onto the subspace.\n",
    "\n",
    "### How to choose the dimension $M$ of the subspace?\n",
    "A rule of thumb is to choose the smallest $M$ such that \"99\\% of the variance is still retained\". That is, the relative variance loss is\n",
    "$$J_r\\leq0.01.$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
